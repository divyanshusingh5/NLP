{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf1d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3594445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumy=\"We perform open vocabulary detection of the objects mentioned in the sentence using both bottom-up and top-down feedback. Object detection is the fundamental computer vision task of finding all “objects” that are present in a visual scene. However, this raises the question, what is an object? Typically, this question is side-stepped by defining a vocabulary of categories and then training a model to detect instances of this vocabulary. This means that if “apple” is not in this vocabulary, the model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2ba355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2524e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=sent_tokenize(dumy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b500823d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We perform open vocabulary detection of the objects mentioned in the sentence using both bottom-up and top-down feedback.',\n",
       " 'Object detection is the fundamental computer vision task of finding all “objects” that are present in a visual scene.',\n",
       " 'However, this raises the question, what is an object?',\n",
       " 'Typically, this question is side-stepped by defining a vocabulary of categories and then training a model to detect instances of this vocabulary.',\n",
       " 'This means that if “apple” is not in this vocabulary, the model']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb23388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'perform',\n",
       " 'open',\n",
       " 'vocabulary',\n",
       " 'detection',\n",
       " 'of',\n",
       " 'the',\n",
       " 'objects',\n",
       " 'mentioned',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'using',\n",
       " 'both',\n",
       " 'bottom-up',\n",
       " 'and',\n",
       " 'top-down',\n",
       " 'feedback',\n",
       " '.',\n",
       " 'Object',\n",
       " 'detection',\n",
       " 'is',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'computer',\n",
       " 'vision',\n",
       " 'task',\n",
       " 'of',\n",
       " 'finding',\n",
       " 'all',\n",
       " '“',\n",
       " 'objects',\n",
       " '”',\n",
       " 'that',\n",
       " 'are',\n",
       " 'present',\n",
       " 'in',\n",
       " 'a',\n",
       " 'visual',\n",
       " 'scene',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'this',\n",
       " 'raises',\n",
       " 'the',\n",
       " 'question',\n",
       " ',',\n",
       " 'what',\n",
       " 'is',\n",
       " 'an',\n",
       " 'object',\n",
       " '?',\n",
       " 'Typically',\n",
       " ',',\n",
       " 'this',\n",
       " 'question',\n",
       " 'is',\n",
       " 'side-stepped',\n",
       " 'by',\n",
       " 'defining',\n",
       " 'a',\n",
       " 'vocabulary',\n",
       " 'of',\n",
       " 'categories',\n",
       " 'and',\n",
       " 'then',\n",
       " 'training',\n",
       " 'a',\n",
       " 'model',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'instances',\n",
       " 'of',\n",
       " 'this',\n",
       " 'vocabulary',\n",
       " '.',\n",
       " 'This',\n",
       " 'means',\n",
       " 'that',\n",
       " 'if',\n",
       " '“',\n",
       " 'apple',\n",
       " '”',\n",
       " 'is',\n",
       " 'not',\n",
       " 'in',\n",
       " 'this',\n",
       " 'vocabulary',\n",
       " ',',\n",
       " 'the',\n",
       " 'model']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds=word_tokenize(dumy)\n",
    "wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2e1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'perform', 'open', 'vocabulary', 'detection', 'of', 'the', 'objects', 'mentioned', 'in', 'the', 'sentence', 'using', 'both', 'bottom-up', 'and', 'top-down', 'feedback', '.']\n",
      "['Object', 'detection', 'is', 'the', 'fundamental', 'computer', 'vision', 'task', 'of', 'finding', 'all', '“', 'objects', '”', 'that', 'are', 'present', 'in', 'a', 'visual', 'scene', '.']\n",
      "['However', ',', 'this', 'raises', 'the', 'question', ',', 'what', 'is', 'an', 'object', '?']\n",
      "['Typically', ',', 'this', 'question', 'is', 'side-stepped', 'by', 'defining', 'a', 'vocabulary', 'of', 'categories', 'and', 'then', 'training', 'a', 'model', 'to', 'detect', 'instances', 'of', 'this', 'vocabulary', '.']\n",
      "['This', 'means', 'that', 'if', '“', 'apple', '”', 'is', 'not', 'in', 'this', 'vocabulary', ',', 'the', 'model']\n"
     ]
    }
   ],
   "source": [
    "for i in sents:\n",
    "    print(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2a0a5",
   "metadata": {},
   "source": [
    "# 1.Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68583cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb8801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014d8da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef6fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data[\"review\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7d1f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "580cbe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b787ce2",
   "metadata": {},
   "source": [
    "Apply lower Casing on whole data Set\n",
    "\n",
    "data[\"review\"]=data[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c5df9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"]=data[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "899e9f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. <br /><br />the...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3651bde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phil the alien is one of those quirky films wh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i saw this movie when i was about 12 when it c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>this is your typical junk comedy.&lt;br /&gt;&lt;br /&gt;t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "7      this show was an amazing, fresh & innovative i...  negative\n",
       "8      encouraged by the positive comments about this...  negative\n",
       "10     phil the alien is one of those quirky films wh...  negative\n",
       "11     i saw this movie when i was about 12 when it c...  negative\n",
       "...                                                  ...       ...\n",
       "49994  this is your typical junk comedy.<br /><br />t...  negative\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"sentiment\"]==\"negative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678dc25",
   "metadata": {},
   "source": [
    "Remove unimportant stuff \n",
    "\n",
    "# 2)HTML tags remove By simply using Regular expression tools like reg ex one one onne past string for pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eaed3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1352320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7cdc669",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"]=data[\"review\"].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec4d27e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. the filming tec...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ef3f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"review\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75ddf3",
   "metadata": {},
   "source": [
    "# 3. Remove URLS again using Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a3e2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"https://www.youtube.com/watch?v=6C0sLtw5ctc&list=PLKnIA16_RmvZo7fp5kkIth6nRTeQQsjfX&index=3&ab_channel=CampusX\"\n",
    "text2=\"Check the link for learning mr https://www.youtube.com/watch?v=6C0sLtw5ctc&list=PLKnIA16_RmvZo7fp5kkIth6nRTeQQsjfX&index=3&ab_channel=CampusX mr divy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ffc50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3=\"www.google.cm Search here\"\n",
    "text4=\"Check the link for learning mr https://www.youtube.com/watch?v=6C0sLtw5ctc&list=PLKnIA16_RmvZo7fp5kkIth6nRTeQQsjfX&index=3&ab_channel=CampusX mr divy and use google for www.google.cm Search here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "149d6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make patterns paste string in online platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9584bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+');\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "386330c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da8d25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text5=\"Check this text5 part printed not is regular exp https://www.youtube.com/watch?v=6C0sLtw5ctc&list=PLKnIA16_RmvZo7fp5kkIth6nRTeQQsjfX&index=3&ab_channel=CampusX\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e57fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "566d4c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check the link for learning mr  mr divy'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7437a092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Search here'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14e83115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check the link for learning mr  mr divy and use google for  Search here'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207230e",
   "metadata": {},
   "source": [
    "# 4. Remove Punctuations from reviews\n",
    "\n",
    "Python considers these as punctaution marks\n",
    "\n",
    "\" !\"#$%&|'()*+,-./:;<=>?@[\\\\]^_`{|}~ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e7b2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we dont want particular symbol as punctuation we can remove from list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac5a4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why we need to remove punctutuion\"\n",
    "import string,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "104d7bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7f1fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8702a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9f0bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1=' string.With.Punctuation?'\n",
    "t2=\"txt1=' string.With.Punctuation??/**'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649cc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36b893db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " stringWithPunctuation\n",
      "txt1 stringWithPunctuation\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print(remove_punc(txt1))\n",
    "print(remove_punc(t2))\n",
    "end=time.time()\n",
    "print((end-start)*5000000)\n",
    "#here i m getting error it should consume some time which is high "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e40f1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it used to be slow using function  to remove punctution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275eb1c",
   "metadata": {},
   "source": [
    "Method 2 to remove Punctutuion marks which is less time consuming then tradinal function making "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c0a1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use time code \n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b17bab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt1 stringWithPunctuation\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print(remove_punc1(t2))\n",
    "time2=time.time()-start\n",
    "print(time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf68c7f",
   "metadata": {},
   "source": [
    "# 5.Chat Words Treat-Ment \n",
    "\n",
    "Example- Lmao, Gn,fyi,ASAP, stfu,, etc:..\n",
    "mostly social media apps or chat datas \n",
    "\n",
    "1st replace these words with normal form \n",
    "for this we need just a dictionary of it \n",
    "\n",
    "\n",
    "here below there is detail of around 60 short hand \n",
    "\n",
    "chat_words={'AFAIK':'As Far As I Know',\n",
    "            'AFK':'Away From Keyboard',\n",
    "            'ASAP':'As Soon As Possible',\n",
    "            'ATK':'At The Keyboard',\n",
    "            'ATM':'At The Moment',\n",
    "            'A3':'Anytime Anywhere Anyplace',\n",
    "            'BAK':'Back At Keyboard',\n",
    "            'BBL':'Be Back Later',\n",
    "            'BBS':'Be Back Soon',\n",
    "            'BFN':'Bye For Now'\n",
    "            ,'B4N':'Bye For Now','BRB':'Be Right Back'\n",
    "            ,'BRT':'Be Right There',\n",
    "            'BTW':'By The Way',\n",
    "            'B4':'Before',\n",
    "            'B4N':'Bye For Now CU',\n",
    "            'CYA':'See You',\n",
    "            'FAQ':'Frequently Asked Questions',\n",
    "            'FC':'Fingers Crossed',\n",
    "            'FWIW':'For What Its Worth ',\n",
    "            'FYI':'For Your Information',\n",
    "            'GAL':'Get A Life',\n",
    "            'GG':'Good Game',\n",
    "            'GN':'Good Night',\n",
    "            'GMTA':'Great Minds Think Alike',\n",
    "            'GR8':'Great!',\n",
    "            'G9':'Genius',\n",
    "            'IC':'I See',\n",
    "            'ICQ':'I Seek you (also a chat program)',\n",
    "            'ILU':'ILU: I Love You',\n",
    "            'IMHO':'In My Honest/Humble Opinion',\n",
    "            'IMO':'In My Opinion',\n",
    "            'IOW':'In Other Words',\n",
    "            'IRL':'In Real Life',\n",
    "            'KISS':'Keep It Simple Stupid',\n",
    "            'LDR':'Long Distance Relationship',\n",
    "            'LMAO':'Laugh My A Off',\n",
    "            'LOL':'Laughing Out Loud',\n",
    "            'LTNS':'Long Time No See',\n",
    "            'L8R':'Late',\n",
    "            'MTE':'My Thoughts Exactly',\n",
    "            'M8':'Mate',\n",
    "            'NRN':'No Reply Necessary',\n",
    "            'OIC':'Oh I See',\n",
    "            'PITA':'Pain In The',\n",
    "            'PRW':'Parents Are Watching',\n",
    "            'ROFL':'Rolling On The Floor Laughing',\n",
    "            'ROFLOL':'Rolling On The Floor Laughing Out Loud',\n",
    "            'ROTFLMAO':'Rolling On The Floor Laughing My A.. Off',\n",
    "            'SK8':'Skate','STATS':'Your sex and age',\n",
    "            'ASL':'Age, Sex, Location','THX':'Thank You',\n",
    "            'TTFN':'Ta-Ta For Now!',\n",
    "            'TTYL':'Talk To You Later',\n",
    "            'U':'You',\n",
    "            'U2':'You Too',\n",
    "            'U4E':'Yours For Ever',\n",
    "            'WB':'Welcome Back','WTF':'What The F...',\n",
    "            'WTG':'Way To Go!',\n",
    "            'WUF':'Where Are You From?',\n",
    "            'W8':'Wait...',\n",
    "            '7K':'Sick:-D Laugher'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c499624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words={'AFAIK':'As Far As I Know',\n",
    "            'AFK':'Away From Keyboard',\n",
    "            'ASAP':'As Soon As Possible',\n",
    "            'ATK':'At The Keyboard',\n",
    "            'ATM':'At The Moment',\n",
    "            'A3':'Anytime Anywhere Anyplace',\n",
    "            'BAK':'Back At Keyboard',\n",
    "            'BBL':'Be Back Later',\n",
    "            'BBS':'Be Back Soon',\n",
    "            'BFN':'Bye For Now'\n",
    "            ,'B4N':'Bye For Now','BRB':'Be Right Back'\n",
    "            ,'BRT':'Be Right There',\n",
    "            'BTW':'By The Way',\n",
    "            'B4':'Before',\n",
    "            'B4N':'Bye For Now CU',\n",
    "            'CYA':'See You',\n",
    "            'FAQ':'Frequently Asked Questions',\n",
    "            'FC':'Fingers Crossed',\n",
    "            'FWIW':'For What Its Worth ',\n",
    "            'FYI':'For Your Information',\n",
    "            'GAL':'Get A Life',\n",
    "            'GG':'Good Game',\n",
    "            'GN':'Good Night',\n",
    "            'GMTA':'Great Minds Think Alike',\n",
    "            'GR8':'Great!',\n",
    "            'G9':'Genius',\n",
    "            'IC':'I See',\n",
    "            'ICQ':'I Seek you (also a chat program)',\n",
    "            'ILU':'ILU: I Love You',\n",
    "            'IMHO':'In My Honest/Humble Opinion',\n",
    "            'IMO':'In My Opinion',\n",
    "            'IOW':'In Other Words',\n",
    "            'IRL':'In Real Life',\n",
    "            'KISS':'Keep It Simple Stupid',\n",
    "            'LDR':'Long Distance Relationship',\n",
    "            'LMAO':'Laugh My A Off',\n",
    "            'LOL':'Laughing Out Loud',\n",
    "            'LTNS':'Long Time No See',\n",
    "            'L8R':'Late',\n",
    "            'MTE':'My Thoughts Exactly',\n",
    "            'M8':'Mate',\n",
    "            'NRN':'No Reply Necessary',\n",
    "            'OIC':'Oh I See',\n",
    "            'PITA':'Pain In The',\n",
    "            'PRW':'Parents Are Watching',\n",
    "            'ROFL':'Rolling On The Floor Laughing',\n",
    "            'ROFLOL':'Rolling On The Floor Laughing Out Loud',\n",
    "            'ROTFLMAO':'Rolling On The Floor Laughing My A.. Off',\n",
    "            'SK8':'Skate','STATS':'Your sex and age',\n",
    "            'ASL':'Age, Sex, Location','THX':'Thank You',\n",
    "            'TTFN':'Ta-Ta For Now!',\n",
    "            'TTYL':'Talk To You Later',\n",
    "            'U':'You',\n",
    "            'U2':'You Too',\n",
    "            'U4E':'Yours For Ever',\n",
    "            'WB':'Welcome Back','WTF':'What The F...',\n",
    "            'WTG':'Way To Go!',\n",
    "            'WUF':'Where Are You From?',\n",
    "            'W8':'Wait...',\n",
    "            '7K':'Sick:-D Laugher'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "055af191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9348a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In My Honest/Humble Opinion he is the best'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion(\"IMHO he is the best \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "538ed7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For Your Information Ntl is best place to learn in srm'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('FYI Ntl is best place to learn in srm ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df0b71",
   "metadata": {},
   "source": [
    "# 6.Spelling Correction\n",
    "Example Please read the notebook and use this ntebook \n",
    "in future on tokenization two words notebook and ntebook will become two different words which willl be useless\n",
    "\n",
    "Multiple techniques are there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a2d97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common type of spelling mistakes it can handle easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4248bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4ca1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_text='Ceertain conditionsa duringg several ggenarations ared moodified in the same maner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e12e3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certain conditions during several generations are modified in the same manner'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textBlb=TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06a4c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn yourself how to apply TextBlob correct on whole data-Frame \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7a586",
   "metadata": {},
   "source": [
    "# 7.Removing Stop words\n",
    "such as a, the ,of , are ,my  we use nltk libraries but for some task like Pos tagging we dont remov stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c620c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a80a0475",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc350057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe77cc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Probaly   match   time fav movie   story       '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('Probaly with this match my all time fav movie is a story of but this and that is not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccf21729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59c1bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"review\"]=data[\"review\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05654fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo\n"
     ]
    }
   ],
   "source": [
    "print(\"helo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcc29a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#USE \n",
    " ['overview'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a6a94",
   "metadata": {},
   "source": [
    "# 8 Handling Emojis\n",
    "\n",
    "1st Remove it with Texxtual data \n",
    "or Replace  it with Meaning \n",
    "use below code  usng regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cd7530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8243593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "622246b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How did you do so , it was funny  '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emojis(\"How did you do so , it was funny  😂\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd416c",
   "metadata": {},
   "source": [
    "How we will replace emoji with meaning \n",
    "\n",
    "by module emoji and use demojize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85fe53b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e30cc880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :face_with_tears_of_joy: to use\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Python is 😂 to use'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "781944e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I m in :pleading_face::smiling_face_with_heart-eyes: :smiling_face_with_heart-eyes:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('I m in 🥺😍 😍'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234221e",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e243a",
   "metadata": {},
   "source": [
    "1ST METHOD DIRECTLY use split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09c19080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1=\"I am going to delhi\"\n",
    "sen1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b29a2ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i m goinging to delh', ' I will call my mom and will have advice ']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen2=\"i m goinging to delh. I will call my mom and will have advice \"\n",
    "sen2.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e40bfbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3='I am going to delhi!'\n",
    "sent3.split()\n",
    "#delhi! is together and will cause trouble in future featear engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f1159",
   "metadata": {},
   "source": [
    "2. Using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dde2102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sen3=\"I m going to delhi!\"\n",
    "tokens=re.findall(\"[\\w']+\",sen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "052c1e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'm', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ccce2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what is we wanted delhi and ! as two separate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0a678",
   "metadata": {},
   "source": [
    "#                    3   Using Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf0b14",
   "metadata": {},
   "source": [
    "i. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63a1a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cbf1f482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'm', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1=\"I m going to visit delhi!\"\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc83ed58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama!\",\n",
       " 'as a drama the movie is watchable.',\n",
       " 'parents are divorcing & arguing like in real life.',\n",
       " 'and then we have jake with his closet which totally ruins all the film!',\n",
       " 'i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs.',\n",
       " 'as for the shots with jake: just ignore them.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2=\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\"\n",
    "sent_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "626c401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent4='I have a ph.d in A.I'\n",
    "sent5=\"We' re here to help! mail us at nk@gamil.com\"\n",
    "sent6=\"A 5km ride cost $10.5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "94475651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'ph.d', 'in', 'A.I']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ab56e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'\",\n",
       " 're',\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nk',\n",
       " '@',\n",
       " 'gamil.com']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent5)\n",
    "# problem it breaks the gamil id here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c68445d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.5']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)\n",
    "# Here also 5km gives error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a463301",
   "metadata": {},
   "source": [
    "   #                                                                using Spacy Library               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4250d48",
   "metadata": {},
   "source": [
    "A better way then NLTk is spacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17a6a691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85e3f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5979ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need To convert sentence in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4830c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(sent5)\n",
    "doc2=nlp(sent1)\n",
    "doc3=nlp(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5521638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.5\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f76b636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'\n",
      "re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nk@gamil.com\n"
     ]
    }
   ],
   "source": [
    "for j in doc1:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fa16a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To apply on whole data-frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05ece232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#data['new_col'] = data['review'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77d12a",
   "metadata": {},
   "source": [
    " # Stemming /Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f486fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stemming we can use porter stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0d582ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5a8363e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243cbe4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PorterStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m \u001b[43mPorterStemmer\u001b[49m()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstemming\u001b[39m(data):\n\u001b[0;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PorterStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(data):\n",
    "    text = [stemmer.stem(word) for word in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff2222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=\"walk walks walking walked\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7be9e39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stem_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstem_words\u001b[49m(sample)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stem_words' is not defined"
     ]
    }
   ],
   "source": [
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d8b33",
   "metadata": {},
   "source": [
    "Sometimes root word is not English words and word without meaning that is invalid word kuch bhi rhta hai so to avoid this when we need to show result of stemmming we use alternative Lemmatization \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ffbe0ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walkkl walkkl walkkkl walkkl'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1=\"walkkl walkkls walkkkling walkkled\"\n",
    "stem_words(sample1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057ecca",
   "metadata": {},
   "source": [
    "Lemmatization n stemming work same but lemmatization root word willl always be a word of english unlike stemming which sometime may give non english wprds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8a869",
   "metadata": {},
   "source": [
    "Disadvantage of Lemmatization is it is bit slow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8271367f",
   "metadata": {},
   "source": [
    "IF WE dont need to show output to user we should use stemming as it fast "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872ef22",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In stemmming there is algo but in lemmatization we see in dictionary like wordnet(lexical dictionary) to find Lemma(root word ) whichmakes it slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e23b3",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ea1fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30ee9884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "time                time                \n",
      ".He                 .He                 \n",
      "has                 ha                  \n",
      "a                   a                   \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "sun                 sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "sen=\"He was running and eating at time .He has a bad habit of swimming after playing long hours in the sun\"\n",
    "punctuation=\"?:!.,;\"\n",
    "sen_words=nltk.word_tokenize(sen)\n",
    "for word in sen_words:\n",
    "    if word in punctuation:\n",
    "        sen_words.remove(word)\n",
    "sen_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sen_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3fef76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can notice nothinghapped because in Lemmatization we need to metion the pos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5db4c028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "time                time                \n",
      ".He                 .He                 \n",
      "has                 have                \n",
      "a                   a                   \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "sun                 sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "sen=\"He was running and eating at time .He has a bad habit of swimming after playing long hours in the sun\"\n",
    "punctuation=\"?:!.,;\"\n",
    "sen_words=nltk.word_tokenize(sen)\n",
    "for word in sen_words:\n",
    "    if word in punctuation:\n",
    "        sen_words.remove(word)\n",
    "sen_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sen_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4a3ed239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eca3ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assignment \n",
    "#Create data set perform text preprocessing , multiclass classification-movies data set description of movie column 2nd jounor [name description genere(action, adventure ,..)]\n",
    "#using TMDB api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dfcc4f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. the filming tec...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9108f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One HOt encoding - not used because of ---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb2df7",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf1a14",
   "metadata": {},
   "source": [
    "# BAG Of Words\n",
    "work best on TEXT-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12858856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bee2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e334ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'text':['people watch campusx','campusx watch campusx','people write comment','campusx write comment'],'output':[1,1,0,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2349204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campusx watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>campusx write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch campusx       1\n",
       "1  campusx watch campusx       1\n",
       "2   people write comment       0\n",
       "3  campusx write comment       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba489b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5e51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow =cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053430e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}\n"
     ]
    }
   ],
   "source": [
    "#vocab\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327466de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=bow[1].toarray()\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7937c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([\"campusx watch and write comment of campusx is collage\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f698245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#automatically handled "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f97dfe",
   "metadata": {},
   "source": [
    "N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62643a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of word is special cas eof N gram=unigram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d421c",
   "metadata": {},
   "source": [
    "Bi-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f067d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(ngram_range=(2,2))# Step for Bi-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "842d9e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cbc80ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people watch': 2, 'watch campusx': 4, 'campusx watch': 0, 'people write': 3, 'write comment': 5, 'campusx write': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49565f14",
   "metadata": {},
   "source": [
    "Tri-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "656f203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(ngram_range=(3,3))#In this case Unigrams and Bi gram bith will be there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b7031ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "787b01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01dbd56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people watch campusx': 2, 'campusx watch campusx': 0, 'people write comment': 3, 'campusx write comment': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f00c6",
   "metadata": {},
   "source": [
    "N-Gram (to include aal the bigram , one gram and every thing )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79405ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "326e1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8542ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20ceeeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 4, 'watch': 7, 'campusx': 0, 'people watch': 5, 'watch campusx': 8, 'campusx watch': 1, 'write': 9, 'comment': 3, 'people write': 6, 'write comment': 10, 'campusx write': 2}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39c0d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc73d2",
   "metadata": {},
   "source": [
    "# TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b665d9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py4tfidf\n",
      "  Downloading py4tfidf-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: py4tfidf\n",
      "Successfully installed py4tfidf-0.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install py4tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0064a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc57f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1633fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49681612, 0.        , 0.61366674, 0.61366674, 0.        ],\n",
       "       [0.8508161 , 0.        , 0.        , 0.52546357, 0.        ],\n",
       "       [0.        , 0.57735027, 0.57735027, 0.        , 0.57735027],\n",
       "       [0.49681612, 0.61366674, 0.        , 0.        , 0.61366674]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit_transform(df['text']).toarray()#idf value we add 1 so term is not totally ignored because on multiply with tf whole contribution become zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d1bf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.22314355 1.51082562 1.51082562 1.51082562 1.51082562]\n",
      "<bound method CountVectorizer.get_feature_names_out of TfidfVectorizer()>\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idf_)\n",
    "print(tfidf.get_feature_names_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79fd37d",
   "metadata": {},
   "source": [
    "## Why we consider Log while taking IDF \n",
    "For rare value IDF vallue may increase eventually we have tto multiply with tf value btween 0 and 1 so idf value will donminate and we want both importance \n",
    "TF say- vo word us document me kitna Important hai \n",
    "IDF say- Vo word pure corpus me kitna important hai "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff951e",
   "metadata": {},
   "source": [
    "# Disadavantage \n",
    "Sparcity\n",
    "Out of Vocabulary\n",
    "dimension \n",
    "sementatic relationship cant be captured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4a84b",
   "metadata": {},
   "source": [
    "### Custom Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe5ff5",
   "metadata": {},
   "source": [
    "## Word Embedding \n",
    "\n",
    "In Nlp word embedding is a term used for the Representation of words for text analysis , Typically in for of real valued vectors that encodes the meaning of the word such that the word that are closer in the vector space are expected to be similar in meaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb763d",
   "metadata": {},
   "source": [
    " We can use google prtrained model on 3 billion data on gogle new consists of 300 doimensional words for 3 million words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d22b54",
   "metadata": {},
   "source": [
    "# Using Pretrained gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcdc36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44e80834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "246453c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40da6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8644a845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.67187500e-01, -1.21582031e-01,  2.85156250e-01,  8.15429688e-02,\n",
       "        3.19824219e-02, -3.19824219e-02,  1.34765625e-01, -2.73437500e-01,\n",
       "        9.46044922e-03, -1.07421875e-01,  2.48046875e-01, -6.05468750e-01,\n",
       "        5.02929688e-02,  2.98828125e-01,  9.57031250e-02,  1.39648438e-01,\n",
       "       -5.41992188e-02,  2.91015625e-01,  2.85156250e-01,  1.51367188e-01,\n",
       "       -2.89062500e-01, -3.46679688e-02,  1.81884766e-02, -3.92578125e-01,\n",
       "        2.46093750e-01,  2.51953125e-01, -9.86328125e-02,  3.22265625e-01,\n",
       "        4.49218750e-01, -1.36718750e-01, -2.34375000e-01,  4.12597656e-02,\n",
       "       -2.15820312e-01,  1.69921875e-01,  2.56347656e-02,  1.50146484e-02,\n",
       "       -3.75976562e-02,  6.95800781e-03,  4.00390625e-01,  2.09960938e-01,\n",
       "        1.17675781e-01, -4.19921875e-02,  2.34375000e-01,  2.03125000e-01,\n",
       "       -1.86523438e-01, -2.46093750e-01,  3.12500000e-01, -2.59765625e-01,\n",
       "       -1.06933594e-01,  1.04003906e-01, -1.79687500e-01,  5.71289062e-02,\n",
       "       -7.41577148e-03, -5.59082031e-02,  7.61718750e-02, -4.14062500e-01,\n",
       "       -3.65234375e-01, -3.35937500e-01, -1.54296875e-01, -2.39257812e-01,\n",
       "       -3.73046875e-01,  2.27355957e-03, -3.51562500e-01,  8.64257812e-02,\n",
       "        1.26953125e-01,  2.21679688e-01, -9.86328125e-02,  1.08886719e-01,\n",
       "        3.65234375e-01, -5.66406250e-02,  5.66406250e-02, -1.09375000e-01,\n",
       "       -1.66992188e-01, -4.54101562e-02, -2.00195312e-01, -1.22558594e-01,\n",
       "        1.31835938e-01, -1.31835938e-01,  1.03027344e-01, -3.41796875e-01,\n",
       "       -1.57226562e-01,  2.04101562e-01,  4.39453125e-02,  2.44140625e-01,\n",
       "       -3.19824219e-02,  3.20312500e-01, -4.41894531e-02,  1.08398438e-01,\n",
       "       -4.98046875e-02, -9.52148438e-03,  2.46093750e-01, -5.59082031e-02,\n",
       "        4.07714844e-02, -1.78222656e-02, -2.95410156e-02,  1.65039062e-01,\n",
       "        5.03906250e-01, -2.81250000e-01,  9.81445312e-02,  1.80664062e-02,\n",
       "       -1.83593750e-01,  2.53906250e-01,  2.25585938e-01,  1.63574219e-02,\n",
       "        1.81640625e-01,  1.38671875e-01,  3.33984375e-01,  1.39648438e-01,\n",
       "        1.45874023e-02, -2.89306641e-02, -8.39843750e-02,  1.50390625e-01,\n",
       "        1.67968750e-01,  2.28515625e-01,  3.59375000e-01,  1.22558594e-01,\n",
       "       -3.28125000e-01, -1.56250000e-01,  2.77343750e-01,  1.77001953e-02,\n",
       "       -1.46484375e-01, -4.51660156e-03, -4.46777344e-02,  1.75781250e-01,\n",
       "       -3.75000000e-01,  1.16699219e-01, -1.39648438e-01,  2.55859375e-01,\n",
       "       -1.96289062e-01, -2.57568359e-02, -5.41992188e-02, -2.51464844e-02,\n",
       "       -1.93359375e-01, -3.17382812e-02, -8.74023438e-02, -1.32812500e-01,\n",
       "       -2.12402344e-02,  4.33593750e-01, -5.20019531e-02,  3.46679688e-02,\n",
       "        8.00781250e-02,  3.41796875e-02,  1.99218750e-01, -2.39257812e-02,\n",
       "       -2.37304688e-01,  1.93359375e-01,  7.32421875e-02, -2.87109375e-01,\n",
       "        1.25000000e-01,  8.44726562e-02,  1.30859375e-01, -2.19726562e-01,\n",
       "       -1.61132812e-01, -2.63671875e-01, -5.46875000e-01, -2.96875000e-01,\n",
       "        3.44238281e-02, -2.87109375e-01, -1.93359375e-01, -1.61132812e-01,\n",
       "       -3.84765625e-01, -2.14843750e-01, -6.22558594e-03, -1.27929688e-01,\n",
       "       -1.00097656e-01, -6.21093750e-01,  3.78906250e-01, -4.58984375e-01,\n",
       "        1.44531250e-01, -9.13085938e-02, -3.08593750e-01,  2.23632812e-01,\n",
       "        7.86132812e-02, -2.16796875e-01,  8.78906250e-02, -1.66992188e-01,\n",
       "        1.14746094e-02, -2.53906250e-01, -6.25000000e-02,  6.04248047e-03,\n",
       "        1.56250000e-01,  4.37500000e-01, -2.23632812e-01, -2.32421875e-01,\n",
       "        2.75390625e-01,  2.39257812e-01,  4.49218750e-02, -7.51953125e-02,\n",
       "        5.74218750e-01, -2.61230469e-02, -1.21582031e-01,  2.44140625e-01,\n",
       "       -3.37890625e-01,  8.59375000e-02, -7.71484375e-02,  4.85839844e-02,\n",
       "        1.43554688e-01,  4.25781250e-01, -4.29687500e-02, -1.08398438e-01,\n",
       "        1.19628906e-01, -1.91406250e-01, -2.12890625e-01, -2.87109375e-01,\n",
       "       -1.14746094e-01, -2.04101562e-01, -2.06298828e-02, -2.53906250e-01,\n",
       "        8.25195312e-02, -3.97949219e-02, -1.57226562e-01,  1.34765625e-01,\n",
       "        2.08007812e-01, -1.78710938e-01, -2.00195312e-02, -8.34960938e-02,\n",
       "       -1.20605469e-01,  4.29687500e-02, -1.94335938e-01, -1.32812500e-01,\n",
       "       -2.17285156e-02, -2.35351562e-01, -3.63281250e-01,  1.51367188e-01,\n",
       "        9.32617188e-02,  1.63085938e-01,  1.02050781e-01, -4.27734375e-01,\n",
       "        2.83203125e-01,  2.74658203e-04, -3.20312500e-01,  1.68457031e-02,\n",
       "        4.06250000e-01, -5.24902344e-02,  7.91015625e-02, -1.41601562e-01,\n",
       "        5.27343750e-01, -1.26953125e-01,  4.74609375e-01, -6.64062500e-02,\n",
       "        3.41796875e-01, -1.78710938e-01,  3.69140625e-01, -2.05078125e-01,\n",
       "        5.82885742e-03, -1.84570312e-01, -8.88671875e-02, -1.81640625e-01,\n",
       "       -4.80957031e-02,  4.39453125e-01,  2.12890625e-01, -3.07617188e-02,\n",
       "        9.32617188e-02,  2.40234375e-01,  2.39257812e-01,  2.51953125e-01,\n",
       "       -1.98974609e-02,  1.24511719e-01, -4.73632812e-02, -2.13623047e-02,\n",
       "        3.12500000e-02,  3.05175781e-02,  2.79296875e-01,  9.08203125e-02,\n",
       "       -2.02148438e-01, -2.19726562e-02, -2.63671875e-01,  8.78906250e-02,\n",
       "       -1.07421875e-01, -2.49023438e-01, -1.22070312e-02,  1.73828125e-01,\n",
       "       -9.91210938e-02,  7.27539062e-02,  2.59765625e-01, -4.60937500e-01,\n",
       "        3.59375000e-01, -2.25585938e-01,  1.87988281e-02, -2.19726562e-01,\n",
       "       -2.08984375e-01, -1.51367188e-01,  8.64257812e-02,  1.11694336e-02,\n",
       "        6.93359375e-02, -2.99072266e-02,  1.43554688e-01,  1.89453125e-01,\n",
       "       -1.32812500e-01,  4.72656250e-01, -1.40625000e-01, -2.52685547e-02,\n",
       "        1.91406250e-01, -2.63671875e-01, -1.39648438e-01,  1.09375000e-01,\n",
       "        1.97753906e-02,  2.49023438e-01, -1.42578125e-01,  4.15039062e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['cricket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "891b7077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32617188,  0.13085938,  0.03466797, -0.08300781,  0.08984375,\n",
       "       -0.04125977, -0.19824219,  0.00689697,  0.14355469,  0.0019455 ,\n",
       "        0.02880859, -0.25      , -0.08398438, -0.15136719, -0.10205078,\n",
       "        0.04077148, -0.09765625,  0.05932617,  0.02978516, -0.10058594,\n",
       "       -0.13085938,  0.001297  ,  0.02612305, -0.27148438,  0.06396484,\n",
       "       -0.19140625, -0.078125  ,  0.25976562,  0.375     , -0.04541016,\n",
       "        0.16210938,  0.13671875, -0.06396484, -0.02062988, -0.09667969,\n",
       "        0.25390625,  0.24804688, -0.12695312,  0.07177734,  0.3203125 ,\n",
       "        0.03149414, -0.03857422,  0.21191406, -0.00811768,  0.22265625,\n",
       "       -0.13476562, -0.07617188,  0.01049805, -0.05175781,  0.03808594,\n",
       "       -0.13378906,  0.125     ,  0.0559082 , -0.18261719,  0.08154297,\n",
       "       -0.08447266, -0.07763672, -0.04345703,  0.08105469, -0.01092529,\n",
       "        0.17480469,  0.30664062, -0.04321289, -0.01416016,  0.09082031,\n",
       "       -0.00927734, -0.03442383, -0.11523438,  0.12451172, -0.0246582 ,\n",
       "        0.08544922,  0.14355469, -0.27734375,  0.03662109, -0.11035156,\n",
       "        0.13085938, -0.01721191, -0.08056641, -0.00708008, -0.02954102,\n",
       "        0.30078125, -0.09033203,  0.03149414, -0.18652344, -0.11181641,\n",
       "        0.10253906, -0.25976562, -0.02209473,  0.16796875, -0.05322266,\n",
       "       -0.14550781, -0.01049805, -0.03039551, -0.03857422,  0.11523438,\n",
       "       -0.0062561 , -0.13964844,  0.08007812,  0.06103516, -0.15332031,\n",
       "       -0.11132812, -0.14160156,  0.19824219, -0.06933594,  0.29296875,\n",
       "       -0.16015625,  0.20898438,  0.00041771,  0.01831055, -0.20214844,\n",
       "        0.04760742,  0.05810547, -0.0123291 , -0.01989746, -0.00364685,\n",
       "       -0.0135498 , -0.08251953, -0.03149414,  0.00717163,  0.20117188,\n",
       "        0.08300781, -0.0480957 , -0.26367188, -0.09667969, -0.22558594,\n",
       "       -0.09667969,  0.06494141, -0.02502441,  0.08496094,  0.03198242,\n",
       "       -0.07568359, -0.25390625, -0.11669922, -0.01446533, -0.16015625,\n",
       "       -0.00701904, -0.05712891,  0.02807617, -0.09179688,  0.25195312,\n",
       "        0.24121094,  0.06640625,  0.12988281,  0.17089844, -0.13671875,\n",
       "        0.1875    , -0.10009766, -0.04199219, -0.12011719,  0.00524902,\n",
       "        0.15625   , -0.203125  , -0.07128906, -0.06103516,  0.01635742,\n",
       "        0.18261719,  0.03588867, -0.04248047,  0.16796875, -0.15039062,\n",
       "       -0.16992188,  0.01831055,  0.27734375, -0.01269531, -0.0390625 ,\n",
       "       -0.15429688,  0.18457031, -0.07910156,  0.09033203, -0.02709961,\n",
       "        0.08251953,  0.06738281, -0.16113281, -0.19628906, -0.15234375,\n",
       "       -0.04711914,  0.04760742,  0.05908203, -0.16894531, -0.14941406,\n",
       "        0.12988281,  0.04321289,  0.02624512, -0.1796875 , -0.19628906,\n",
       "        0.06445312,  0.08935547,  0.1640625 , -0.03808594, -0.09814453,\n",
       "       -0.01483154,  0.1875    ,  0.12792969,  0.22753906,  0.01818848,\n",
       "       -0.07958984, -0.11376953, -0.06933594, -0.15527344, -0.08105469,\n",
       "       -0.09277344, -0.11328125, -0.15136719, -0.08007812, -0.05126953,\n",
       "       -0.15332031,  0.11669922,  0.06835938,  0.0324707 , -0.33984375,\n",
       "       -0.08154297, -0.08349609,  0.04003906,  0.04907227, -0.24121094,\n",
       "       -0.13476562, -0.05932617,  0.12158203, -0.34179688,  0.16503906,\n",
       "        0.06176758, -0.18164062,  0.20117188, -0.07714844,  0.1640625 ,\n",
       "        0.00402832,  0.30273438, -0.10009766, -0.13671875, -0.05957031,\n",
       "        0.0625    , -0.21289062, -0.06542969,  0.1796875 , -0.07763672,\n",
       "       -0.01928711, -0.15039062, -0.00106049,  0.03417969,  0.03344727,\n",
       "        0.19335938,  0.01965332, -0.19921875, -0.10644531,  0.01525879,\n",
       "        0.00927734,  0.01416016, -0.02392578,  0.05883789,  0.02368164,\n",
       "        0.125     ,  0.04760742, -0.05566406,  0.11572266,  0.14746094,\n",
       "        0.1015625 , -0.07128906, -0.07714844, -0.12597656,  0.0291748 ,\n",
       "        0.09521484, -0.12402344, -0.109375  , -0.12890625,  0.16308594,\n",
       "        0.28320312, -0.03149414,  0.12304688, -0.23242188, -0.09375   ,\n",
       "       -0.12988281,  0.0135498 , -0.03881836, -0.08251953,  0.00897217,\n",
       "        0.16308594,  0.10546875, -0.13867188, -0.16503906, -0.03857422,\n",
       "        0.10839844, -0.10498047,  0.06396484,  0.38867188, -0.05981445,\n",
       "       -0.0612793 , -0.10449219, -0.16796875,  0.07177734,  0.13964844,\n",
       "        0.15527344, -0.03125   , -0.20214844, -0.12988281, -0.10058594,\n",
       "       -0.06396484, -0.08349609, -0.30273438, -0.08007812,  0.02099609],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"man\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f631b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"man\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30e02727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"love\"].shape# vector of 300 dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4795e42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"sex\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef44a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6b87f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"beer\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49146d02",
   "metadata": {},
   "source": [
    "### This 300 words set is a vector representation made up of 300 word or 300 dimensinal space "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8cab9",
   "metadata": {},
   "source": [
    "### we can add two vector to arthematics and do all stuff in word to vec sector section of 300 dimensional vector  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc4254",
   "metadata": {},
   "source": [
    "### most_similar = will internally convert the word in vector and find the similar closest vector(Closest can be find with cos theta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac7f6ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('See', 0.7043843865394592),\n",
       " ('Check', 0.567571759223938),\n",
       " ('Click', 0.5424338579177856),\n",
       " ('div.innerHTML_=_Read', 0.5167247653007507),\n",
       " ('Find', 0.5151839852333069),\n",
       " ('read', 0.5093329548835754),\n",
       " ('Visit', 0.5086786150932312),\n",
       " ('please_visit', 0.48926910758018494),\n",
       " ('www.news##.com', 0.4855078458786011),\n",
       " ('By_Paul_Shread', 0.4829995334148407)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('Read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac3d5ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Air_Force', 0.6859970688819885),\n",
       " ('Marine_Corps', 0.6812339425086975),\n",
       " ('Navy', 0.6537189483642578),\n",
       " ('army', 0.6531062126159668),\n",
       " ('military', 0.6404651999473572),\n",
       " ('Armed_Forces', 0.6334443092346191),\n",
       " ('1st_Cav', 0.6132252812385559),\n",
       " ('Marines', 0.6075754761695862),\n",
       " ('infantry', 0.5990438461303711),\n",
       " ('soldier', 0.5853056311607361)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('Army')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "234deaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7674937"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('men','women')# Will convert the two words in vec and find the cosine similarty between both of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c00b6ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'monkey'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match([\"PHP\",\"Java\",\"monkey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38d0c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can perdorm vector arthematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "222bc9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300517559051514),\n",
       " ('monarch', 0.645466148853302),\n",
       " ('princess', 0.6156251430511475),\n",
       " ('crown_prince', 0.5818676352500916),\n",
       " ('prince', 0.5777117609977722),\n",
       " ('kings', 0.5613663792610168),\n",
       " ('sultan', 0.5376775860786438),\n",
       " ('queens', 0.5289887189865112),\n",
       " ('ruler', 0.5247419476509094)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model['king'] - model['man'] + model['woman']\n",
    "model.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c7c0dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('INR', 0.6442341208457947),\n",
       " ('GBP', 0.5040826797485352),\n",
       " ('England', 0.44649264216423035),\n",
       " ('£', 0.43340998888015747),\n",
       " ('Â_£', 0.4307197630405426),\n",
       " ('£_#.##m', 0.42561301589012146),\n",
       " ('Pounds_Sterling', 0.42512619495391846),\n",
       " ('GBP##', 0.42464491724967957),\n",
       " ('stg', 0.42324796319007874),\n",
       " ('£_#.###m', 0.4201711118221283)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model['INR'] - model ['India'] + model['England']\n",
    "model.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31fffb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beijing', 0.786591649055481),\n",
       " ('Delhi', 0.7299180030822754),\n",
       " ('China', 0.6696014404296875),\n",
       " ('Shanghai', 0.6233492493629456),\n",
       " ('Guangzhou', 0.6118125915527344),\n",
       " ('Shenyang', 0.6010918617248535),\n",
       " ('Bejing', 0.5996053814888),\n",
       " ('Chinese', 0.5894550085067749),\n",
       " ('Guangdong', 0.58624267578125),\n",
       " ('Nanjing', 0.5795078277587891)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model['Delhi'] - model ['India'] + model['China']\n",
    "model.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fb94806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How this now coem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c2c3415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('King', 0.7952353358268738),\n",
       " ('Queen', 0.47166240215301514),\n",
       " ('Greene', 0.4265486001968384),\n",
       " ('Martin_Luther_King', 0.39700499176979065),\n",
       " ('Jackson', 0.3948497772216797),\n",
       " ('monarch', 0.39400187134742737),\n",
       " ('Coretta_King', 0.38876548409461975),\n",
       " ('Princess', 0.38846564292907715),\n",
       " ('Prince', 0.38728970289230347),\n",
       " ('queen', 0.3759741485118866)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model['King'] - model ['men'] + model['women']\n",
    "model.most_similar([vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba16fd",
   "metadata": {},
   "source": [
    "## How to train word2vec model on own library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a36c4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fad49f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "story = []\n",
    "for filename in os.listdir('data'):\n",
    "    \n",
    "    f = open(os.path.join('data',filename))\n",
    "    corpus = f.read()\n",
    "    raw_sent = sent_tokenize(corpus)\n",
    "    for sent in raw_sent:\n",
    "        story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8bda9009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141218"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36383f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['game',\n",
       " 'of',\n",
       " 'thrones',\n",
       " 'book',\n",
       " 'one',\n",
       " 'of',\n",
       " 'song',\n",
       " 'of',\n",
       " 'ice',\n",
       " 'and',\n",
       " 'fire',\n",
       " 'by',\n",
       " 'george',\n",
       " 'martin',\n",
       " 'prologue',\n",
       " 'we',\n",
       " 'should',\n",
       " 'start',\n",
       " 'back',\n",
       " 'gared',\n",
       " 'urged',\n",
       " 'as',\n",
       " 'the',\n",
       " 'woods',\n",
       " 'began',\n",
       " 'to',\n",
       " 'grow',\n",
       " 'dark',\n",
       " 'around',\n",
       " 'them']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f60122c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cce6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## window = 10 means both side will have 10 10 values \n",
    "# min_count=2 ,eans those sen only having 2 words min in sen sentance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ff591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector size of can be change \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "130ff824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(story)# building vocabulary of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7601fc75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6570562, 8628190)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(story, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "# total sentencte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3efdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode.wv we can use our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3e729cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stormborn', 0.7543193697929382),\n",
       " ('targaryen', 0.7104233503341675),\n",
       " ('unburnt', 0.6357699036598206),\n",
       " ('viserys', 0.6187607049942017),\n",
       " ('myrcella', 0.5809696912765503),\n",
       " ('dorne', 0.5782967209815979),\n",
       " ('elia', 0.5647744536399841),\n",
       " ('xaro', 0.563541054725647),\n",
       " ('khaleesi', 0.5602869391441345),\n",
       " ('westeros', 0.5577344298362732)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('daenerys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "677f0ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jon'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['jon','rikon','arya','sansa','bran'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15c9e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jon was adopte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6967a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1551443 , -0.8389296 , -1.1618391 , -3.6432712 , -0.8137444 ,\n",
       "        1.2367631 ,  0.5259793 ,  3.0329182 ,  4.678057  , -0.4823852 ,\n",
       "       -0.36970305,  0.2025816 , -0.91046005, -1.4417095 , -3.5598838 ,\n",
       "        2.1507576 , -1.7631594 , -0.09952141,  1.8068744 , -0.96756047,\n",
       "       -3.5780513 ,  0.9658994 , -1.4330803 ,  1.1415131 , -1.1663738 ,\n",
       "       -0.32346517,  0.88170654, -0.18624194, -1.6231226 ,  1.5500555 ,\n",
       "        0.73553646, -1.9669254 , -0.41351807, -2.3212337 , -0.925055  ,\n",
       "        0.5961323 ,  0.8728762 , -0.93850833, -0.26064616, -3.704214  ,\n",
       "       -1.5798482 ,  3.5194511 , -2.288661  ,  0.53664505, -0.7468959 ,\n",
       "        1.9534492 ,  0.7118651 ,  0.1586993 , -2.279533  , -2.0581343 ,\n",
       "        0.27404404,  0.50609744, -1.1791307 ,  1.1406354 ,  0.2804591 ,\n",
       "       -0.2720502 ,  0.38788477, -0.9036533 , -1.4480081 , -1.4216733 ,\n",
       "       -4.2821136 ,  0.27185613,  0.14022404, -1.0699947 , -1.3028395 ,\n",
       "       -2.244854  ,  1.9338013 , -1.1345245 ,  1.738174  , -2.5488386 ,\n",
       "        0.94004494, -1.3409617 , -0.07968512, -2.015686  , -3.369571  ,\n",
       "       -0.13466899,  1.0984704 , -1.6098145 ,  1.4028771 , -3.5829384 ,\n",
       "       -0.33911303,  1.0206847 , -0.8356112 , -0.10651899,  0.810188  ,\n",
       "       -1.2319351 , -2.6638992 ,  1.2179236 ,  1.457394  , -1.702914  ,\n",
       "       -0.18114467,  0.22690445,  3.8465605 ,  0.1520177 ,  2.6121721 ,\n",
       "        2.1196935 ,  0.8700637 ,  0.5420011 , -0.2816675 , -2.207516  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['jon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f047e898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7774477"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('arya','sansa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e64790e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63871604"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('cersei','sansa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6fa1ef7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20856489"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('tywin','sansa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "302aa1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we converted the got words in 100 dimension bt we cant see and cant even pot the grapp we can use Dimensionality reduction techinique  PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2cebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
